Процедурные vs автоматные парсеры (автомат со стековой памятью)

У всех предыдущих примеров парсера были общие свойства:
1. Код парсинга активный, то есть сам запрашивает продолжение входных данных.
(Такой код принято называть pull-парсером, потому что он "pulls" входной
поток. Противоположностью является push-парсер, который пассивно вызывается
на каждую порцию ввода.)
2. Код парсинга хранит текущее состояние в виде последовательности вызовов
своих функций и значений их локальных переменных, средствами исполняющей
системы.

(NB термины pull parsing и push parsing имеют другой смысл в традициях
обработки XML и как парсеры XML взаимодействуют с клиентами - получателями
данных; не следует смешивать специфику парсеров вообще со спецификой XML.)

Оба этих свойства могут быть неподходящими в ряде применений. Например, работа
для IDE подразумевает быструю реакцию на каждое изменение кода пользователем,
вплоть до реакции на каждое нажатие клавиши (на практике делается
перечитывание по таймауту после последнего нажатия, или какой-то ещё
аналогичный подход). В этом случае производить полный анализ слишком дорого,
и имеет смысл запоминать состояния в промежуточных точках (например, по одной
на экран кода), и продолжать разбор с такого запомненного состояния.

Если какое-то из названных в начале главы свойств, или оба, не годятся для
конкретного применения, нужно переходить к автоматному построению парсера. В
этом случае у парсера есть память, которую можно вынести из логики кода именно
в виде структуры данных, и интерфейс в виде точки входа и вызываемых выходных
действий.
Память обычно представлена в виде текущего значения параметра "основное
состояние" (далее просто "состояние") - это может быть строка, число, объект,
любая другая сущность - и деталей того, в какой структуре грамматики находимся
- обычно эти детали представлены в виде стека потому, что это лучше всего
отражает иерархически вложенную структуру входного потока. В теории CS за
это отвечает тема "автоматы с памятью", конкретнее - автоматы со стековой
памятью.

Один из простейших метод построения такого автомата - взять рукописный парсер
и назначить код состояния (число или имя) каждой точке выполнения, которая
является началом функции, разбирающей элемент грамматики (нетерминал), или
точкой возврата из вызова функции вложенного нетерминала, а историю вызовов
функций до соответствующей точки выполнения кода - перевести в стековую память
состояния. Это будет достаточно неэкономно, тупо, но предельно понятно и
удобно для иллюстрации принципа.

test_mwa_fut_gr02_nrds - без редуцирования набора состояний, прямая трансляция
логики парсера на прямых вызовах - на машину состояний. Он при этом остаётся
нисходящим парсером. Изучите его для понимания именно логики процесса перевода
от парсера на прямых выховах к автоматному парсеру.
Метод parser.run() там содержит цикл, внутренность которого собственно и
является тем, что надо вызывать по одному шагу разбора.
Но этот пример, повторюсь, чудовищно избыточен в плане того, какие требуются
фазы и что сохраняется в стеке. На этом этапе можно уже закрыть эту тему и
перейти к построению более экономных реализаций.

(Следует также упомянуть, что построение push-парсера может быть сделано и
чисто техническими средствами управления выполнением, как отдельные треды
и сопрограммы. Но это не даст восстановление с любой запомненной точки,
поэтому мы не рассматриваем подобный вариант как полезный.)

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Восходящий парсер с состояниями разбора

Теперь переходим к парсеру с более экономным набором состояний.
Принцип построения автоматного парсера можно описать на примере нашего
рукописного нисходящего парсера выражений из чисел (грамматики 1-3). В
варианте без префиксных операций, если посмотреть на его грамматику, можно
увидеть, что в самой первой позиции ожидается только один из двух видов
лексем: число-константа или открывающая круглая скобка. С префиксными
операциями добавится возможность появления префикса ("+" или "-"), но всё
равно мы говорим о вариантах одного и того же unary.
И другой вариант - после полного unary - или инфиксная операция, или конец
ввода (EOI). Здесь это понятно интуитивно; дальше будем разбираться с тем, как
этот факт доказать.
(На самом деле, хорошую подсказку даёт реализация парсера Пратта: как и там,
есть два состояния "надо звать nud" и "надо звать led". Остальные проблемы с
приоритетами решаются контролем предыдущих операций на шаге свёртки. Можно
было бы посчитать это читерством. Но так как мы всё равно, видя стек операций,
не знаем, что в нём находится выше текущей операции... невозможно отражать это
в параметре состояния разбора, потому что набирается бесконечное количество
таких состояний. Приходится всё равно делать переходы состояний в зависимости
от содержания стека.)

Разбираем выражение по грамматике номер 2. Предусловия: выражение корректное,
не пустое. Как и в прошлых случаях, лексер должен давать предпросмотр на одну
лексему, которая каждым конкретным правилом может съедаться или оставляться
нетронутой.

Тогда в таблицу реакций попадает:
(неописанные сочетания вызывают ошибку)

начальное состояние: expect_unary;
элементы стека обозначаются: st[0] - вершина, st[1] - следующий за вершиной,
и так далее;
реакции описываются парами: базовое состояние, текущая лексема;
реакция должна сработать при выполнении обоих условий.

  expect_unary, число =>
    t <- reduce_prefix_ops(число)
    push(t)
    state <- expect_infix_or_eoi
  expect_unary, "(" =>
    push("(")
    state <- expect_unary
  expect_unary, "+" =>
    push(унарный_плюс)
    state <- expect_unary
  expect_unary, "-" =>
    push(унарный_минус)
    state <- expect_unary
  expect_infix_or_eoi, "+" =>
    reduce_infix_ops(all_infix_ops)
    push(infix_plus)
    state <- expect_unary
  expect_infix_or_eoi, "-" =>
    reduce_infix_ops(all_infix_ops)
    push(infix_minus)
    state <- expect_unary
  expect_infix_or_eoi, "*" =>
    reduce_infix_ops(["*", "/", "**"])
    push(infix_star)
    state <- expect_unary
  expect_infix_or_eoi, "/" =>
    reduce_infix_ops(["*", "/", "**"])
    push(infix_slash)
    state <- expect_unary
  expect_infix_or_eoi, "**" =>
    // И снова отличие правой ассоциативности от левой: здесь мы не вызываем
    // редукцию ни для предыдущих "**", ни для каких других операций,
    // удлиняя стек. Хотя, если бы были инфиксные операции
    // с бо́льшим приоритетом, чем "**" - пришлось бы вызвать редукцию для них.
    push(infix_double_star)
    state <- expect_unary
  expect_infix_or_eoi, ")" =>
    reduce_infix_ops(all_infix_ops)
    if st[0] is not number or st[1] is not ")" { error }
    v <- pop()
    pop()
    v <- reduce_prefix_ops(v) // а вот это уже найдено только тестами!
    push(v)
    state <- expect_infix_or_eoi

func reduce_infix_ops(op_set) {
  for(;;) {
    if st[0] is not number { error } // возможно ли?
    elif st[1] in op_set && st[2] is number {
      v2 <- pop()
      op <- pop()
      v1 <- pop()
      t <- op(v1, v2)
      push(t)
    }
    else break
  }
}

func reduce_prefix_ops(v) {
  while (на вершине стека префиксная операция) {
    op <- pop()
    v <- op(v)
  }
  return v
}

В вызове reduce_infix_ops() вызывается свёртка для всех операций с бо́льшим
приоритетом (например, для "+" это умножение, деление, степень), но если
операция левоассоциативна - и для операций того же приоритета (для "+" -
сложение и вычитание); для правоассоциативной свёртываются только операции со
строго бо́льшим приоритетом.

По завершению отработки, в стеке должен оказаться один и только один элемент -
итоговое число.

(полный код парсера с тестами см. в test_mwa_gr02_rds)

Простой пример пошагового выполнения для выражения 3+4*5+6:
внимание - вершина стека тут справа! это произвольное решение.

Начальное состояние: стек: [], состояние: expect_unary.
Принята лексема: 3
результат: стек: [3], состояние: expect_infix_or_eoi.
Принята лексема: +
сворачивать нечего
результат: стек: [3,(infix,"+")], состояние: expect_unary.
Принята лексема: 4
результат: стек: [3,(infix,"+"),4], состояние: expect_infix_or_eoi.
Принята лексема: *
сворачивать нечего (в стеке есть "+", но она менее приоритетна)
результат: стек: [3,(infix,"+"),4,(infix,"*")], состояние: expect_unary.
Принята лексема: 5
результат: стек: [3,(infix,"+"),4,(infix,"*"),5], состояние:
expect_infix_or_eoi.
Принята лексема: +
свёртка обнаруживает ждущую операцию "*" на вершине стека и выполняет её
промежуточный результат: стек: [3,(infix,"+"),20]
свёртка обнаруживает ждущую операцию "+" на вершине стека и выполняет её
промежуточный результат: стек: [23]
результат: стек: [23,(infix,"+")], состояние: expect_unary.
Принята лексема: 6
результат: стек: [23,(infix,"+"),6], состояние: expect_infix_or_eoi.
Принята лексема: EOI
свёртка обнаруживает ждущую операцию "+" на вершине стека и выполняет её
результат: стек: [29]
выполнение закончено.

Заметно, что некоторые решения действия не очевидны. Самое неочевидное это
необходимость вызвать свёртку (reduce) инфиксных операций после отработки
правой круглой скобки; я нашёл это только отладкой с подробной печатью;
большинство, уверен, повторило бы эту же ошибку. Также, универсальность
состояния expect_infix_or_eoi и функция reduce_infix_ops со списком
сокращаемых операций допустима в таком виде по той же причине, по которой
работает парсер Пратта - чёткое следование правил приоритетам операций. В
общем случае мы не смогли бы так сократить.

(XXX привести контрпример)

В общем случае составление таблиц реакций и действий по ним следует отдать
компьютеру. Но тут мы показали, какие они могут быть и как сочетаются - в
таком виде понять это значительно легче, чем в выводе типичных утилит
генерации парсеров, вроде YACC или ANTLR. Следующим этапом должна стать
автоматизация генерации такого кода: набора состояний, таблиц переходов,
кода действий.

Один принципиальный момент в реализации следует упомянуть прямо сейчас:
полученный парсер - не нисходящий (top-down), а восходящий (bottom-up).
Чем же он отличается от нисходящего? Тем, что не исходит от корневого
ожидаемого элемента грамматики, а начинает с "пустого" состояния и оформляет
готовые значения, накапливая результаты и свёртывая их по мере готовности.
Именно отдельное действие условной свёртки является характернейшим признаком
восходящего парсера.
В нисходящем свёртка выполнялась как безусловная часть логики конкретных
грамматических элементов: например, находясь в функции addsub() и увидев
последующее "+", мы вызывали продолжение разбора и складывали результаты
выражения слева и справа от этого оператора. В случае восходящего парсера это
надо делать явной проверкой на условия свёртки: например, имея
последовательность свёрнутых входных данных: 5, "+", 4, и видя, что после 4
идёт что-то, что не может продолжать эту 4 с более высоким приоритетом,
свёртываем 5+4 в 9.

Ещё - тут использовался один общий стек и на числа, и на виды отложенных
операций. Для Python это нормально, потому что типизация динамическая,
и список может содержать данные любых типов. Для языков со статической
типизацией вместо этого лучше поддерживать два стека: для чисел и для операций
(выраженных, например, целыми числами).
Этот вариант сделан в test_mwa_ml_gr02_rds_2s, ничем больше принципиально
не отличается. Сравните эту реализацию с предыдущей.

// vim: set tw=78 et :
